{
	"social_media": [
		{
			"url": "mailto:james.smith deletethis at berkeley.edu",
			"class": "fas fa-envelope",
			"name": "email"
		},
				{
			"url": "https://github.com/jamesdsmith",
			"class": "fab fa-github",
			"name": "github"
		},
		{
			"url": "https://linkedin.com/in/jdsmithcoder",
			"class": "fab fa-linkedin",
			"name": "linked in"
		},
		{
			"url": "https://scholar.google.com/citations?user=qm5jHocAAAAJ&hl=en",
			"class": "ai ai-google-scholar-square",
			"name": "google scholar"
		}
	],
	"research": [
		{
			"title": "Composing Flexibly-Organized Step-by-Step Tutorials from Linked Source Code, Snippets, and Outputs",
			"authors": "Andrew Head, Jason Jiang, <em>James Smith</em>, Marti A. Hearst, Björn Hartmann",
			"description": "Programming tutorials are a pervasive, versatile medium for teaching programming. In this paper, we report on the content and structure of programming tutorials, the pain points authors experience in writing them, and a design for a tool to help improve this process. An interview study with 12 experienced tutorial authors found that they construct documents by interleaving code snippets with text and illustrative outputs. It also revealed that authors must often keep the related artifacts of source programs, snippets, and outputs consistent as a program evolves. A content analysis of 200 frequentlyreferenced tutorials on the web also found that most tutorials contain related artifacts—duplicate code and outputs generated from snippets—that an author would need to keep consistent with each other. To address these needs, we designed a tool called Torii with novel authoring capabilities. An in-lab study showed that tutorial authors can successfully use the tool for the unique affordances identified, and provides guidance for designing future tools for tutorial authoring.",
			"venue": "CHI 2020",
			"paper": "papers/tutorial-authoring.pdf",
			"video": "https://www.youtube.com/watch?v=tTzrBds7Nns",
			"honorable-mention": true,
			"pubonly": true,
			"acmdl": "https://dl.acm.org/doi/abs/10.1145/3313831.3376798",
			"images": [
				{
					"url": "img/torii/tutorial-authoring.jpg",
					"alt": "Diagram showing program execution order.",
					"size": "3"
				}
			]
		},
		{
			"title": "LabelAR: A Spatial Guidance Interface for Fast Computer Vision Image Collection",
			"authors": "Mike Laielli*, <em>James Smith*</em>, Giscard Biamby*, Trevor Darrell, Bjoern Hartmann",
			"description": "Computer vision is applied in an ever expanding range of applications, many of which require custom training data to perform well. We present a novel interface for rapid collection and labeling of training images to improve computer vision based object detectors. <i>LabelAR</i> leverages the spatial tracking capabilities of an AR-enabled camera, allowing users to place persistent bounding volumes that stay centered on real-world objects. The interface then guides the user to move the camera to cover a wide variety of viewpoints. We eliminate the need for post-hoc manual labeling of images by automatically projecting 2D bounding boxes around objects in the images as they are captured from AR-marked viewpoints. In a user study with 12 participants, <i>LabelAR</i> significantly outperforms existing approaches in terms of the trade-off between model performance and collection time.",
			"venue": "UIST 2019",
			"paper": "papers/LabelAR.pdf",
			"website": "http://labelar.berkeley.edu",
			"acmdl": "https://dl.acm.org/doi/abs/10.1145/3332165.3347927",
			"video": "https://www.youtube.com/watch?v=WwYAClL6MlU",
			"images": [
				{
					"url": "img/labelar/user.jpg",
					"alt": "User pointing a cell phone at a cup.",
					"size": "3"
				},
				{
					"url": "img/labelar/labelar-interfaces.jpg",
					"alt": "Cup with holographic cube overlayed. Shows an interface for placing holocubes and also collecting training images.",
					"size": "4"
				},
				{
					"url": "img/labelar/bbox-projection.jpg",
					"alt": "Depicts the 3d reprojection process from holocube to 2d image annotation.",
					"size": "5"
				}
			]
		},
		{
			"title": "HindSight: Enhancing Spatial Awareness by Sonifying Detected Objects in Real-Time 360-Degree Video",
			"description": "<i>HindSight</i> increases the environmental awareness of cyclists by warning them of vehicles approaching from outside their visual field. A panoramic camera mounted on a bicycle helmet streams real-time, 360-degree video to a laptop running YOLOv2, a neural object detector designed for real-time use. Detected vehicles are passed through a filter bank to find the most relevant. Resulting vehicles are sonified using bone conduction headphones, giving cyclists added margin to react.",
			"authors": "Eldon Schoop, <em>James Smith</em>, Bjoern Hartmann",
			"venue": "CHI 2018",
			"paper": "papers/HindSight.pdf",
			"video": "https://www.youtube.com/watch?v=n5ni9usMkD4",
			"acmdl": "https://dl.acm.org/doi/abs/10.1145/3173574.3173717",
			"images": [
				{
					"url": "img/hindsight/donut_diagram_s.jpg",
					"alt": "HindSight Sensing Diagram",
					"size": "4"
				},
				{
					"url": "img/hindsight/lbd_s.jpg",
					"alt": "System Diagram",
					"size": "4"
				},
				{
					"url": "img/hindsight/filtering_stages_s.jpg",
					"alt": "Filtering Stages",
					"size": "4"
				}
			]
		}
	],
	"projects": [
		{
			"title": "VirtuWheel",
			"description": "A major problem with virtual reality is the lack of feedback the user receives during interactions with virtual objects. One solution to this is to use physical props to provide haptic feedback, but this still lacks visual feedback cues that may be helpful or necessary for some tasks. This can negatively affect the user’s sense of embodiment in the virtual space. <i>VirtuWheel</i> provides visual feedback during virtual object interactions by using touch sensors on the surface a steering wheel controller which we can use to visualize interactions with the steering wheel in VR.",
			"authors": "<em>James Smith</em>, Raquel Izquierdo, Caitlin Kwan, Greg Giebel",
			"venue": "Completed for EE249A - Embedded Systems",
			"report": "reports/VirtuWheel.pdf",
			"video": "https://www.youtube.com/watch?v=wzfDXCfKiVs",
			"images": [
				{
					"url": "img/virtuwheel/system_s.png",
					"alt": "Components of the VirtuWheel system",
					"size": "4"
				},
				{
					"url": "img/virtuwheel/vr_s.png",
					"alt": "Visualization of the interaction in virtual reality",
					"size": "4"
				},
				{
					"url": "img/virtuwheel/signal_s.png",
					"alt": "Example of what a signal from our system looks like",
					"size": "4"
				}
			]
		}
	],
	"jobs": [
		{
			"title": "VR Software Engineer", 
			"location": "UC Berkeley", 
			"url": "http://jacobsinstitute.berkeley.edu", 
			"urltext": "Jacobs Institute for Design Innovation",
			"time": "January 2019 - May 2019"
		},
		{
			"title": "Research Assistant", 
			"location": "UC Berkeley", 
			"url": "http://bid.berkeley.edu", 
			"urltext": "Berkeley Institute of Design",
			"time": "January 2018 - May 2018"
		},
		{
			"title": "Undergraduate Research Assistant", 
			"location": "UC Berkeley", 
			"url": "http://bid.berkeley.edu", 
			"urltext": "Berkeley Institute of Design",
			"time": "June 2017 - December 2017"
		},
		{
			"title": "Undergraduate Research Assistant",
			"location": "UC Berkeley",
			"url": "http://www-video.eecs.berkeley.edu/",
			"urltext": "Video and Image Processing Lab",
			"time": "November 2015 - January 2017"
		},
		{
			"title": "Math and Physics Tutor",
			"location": "Sierra College",
			"time": "January 2014 - May 2015"
		},
		{
			"title": "Gameplay Programmer",
			"location": "Sony Online Entertainment",
			"time": "June 2008 - October 2010"
		},
		{
			"title": "Gameplay Programmer",
			"location": "Zombie Studios",
			"time": "May 2005 - June 2008"
		}
	]
}
