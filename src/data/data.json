{
	"social_media": [
		{
			"url": "mailto:james.smith deletethis at berkeley.edu",
			"class": "fas fa-envelope",
			"name": "email"
		},
				{
			"url": "https://github.com/jamesdsmith",
			"class": "fab fa-github",
			"name": "github"
		},
		{
			"url": "https://linkedin.com/in/jdsmithcoder",
			"class": "fab fa-linkedin",
			"name": "linked in"
		},
		{
			"url": "https://scholar.google.com/citations?user=qm5jHocAAAAJ&hl=en",
			"class": "ai ai-google-scholar-square",
			"name": "google scholar"
		}
	],
	"projects": [
		{
			"title": "What's the Game, then? Opportunities and Challenges for Runtime Behavior Generation",
			"authors": [
				{
					"name": "Nicholas Jennings"
				},
				{
					"name": "Han Wang"
				},
				{
					"name": "Isabel Li"
				},
				{
					"name": "James Smith",
					"me": true
				},
				{
					"name": "Bjoern Hartmann"
				}
			],
			"description": "GROMIT is a tool that generates game interactions in real-time, enabling games to adapt to player actions. Built for Unity, it was tested through prototypes like a puzzle escape room, traffic simulation, and adventure game. Developer interviews found the concept promising but raised concerns about quality, player expectations, and workflow integration, suggesting the need for better safeguards.",
			"venue": "UIST 24",
			"pubonly": true,
			"award": "best-paper",
			"acmdl": "https://dl.acm.org/doi/10.1145/3654777.3676358",
			"doi": "https://doi.org/10.1145/3654777.3676358",
			"pdf": "papers/jennings_gromit_24.pdf",
			"images": [],
			"image": {
				"url": "img/gromit-figure1.png",
				"alt": "Gromit system diagram showing the flow of data from the game to the user."
			}
		},
		{
			"title": "iJedi: Improving Bimanual Ray Pointing Performance Through Multiple Bodies",
			"authors": [
				{
					"name": "James Smith",
					"me": true
				},
				{
					"name": "Nicholas Jennings"
				},
				{
					"name": "Bjoern Hartmann"
				}
			],
			"description": "iJedi enhances a VR bimanual interaction technique (iSith) by strategically spacing virtual arms farther apart and aligning them relative to targets in the environment. Testing with 19 participants showed iJedi reduces errors compared to iSith while maintaining similar speed, suggesting that extended body interfaces can optimize complex tasks. The results highlight how virtual body design, like arm placement and spacing, can refine interaction techniques without sacrificing user performance.",
			"venue": "(in submision)",
			"images": [],
			"image": {
				"url": "img/jedi-figure.png",
				"alt": "Diagram showing two avatars in VR, one of the avatars is missing an arm, which appears on the other. Two laser pointers emerge from the arm on each body, and a 3d sphere cursor is places where they intersect."
			}
		},
		{
			"title": "Dual Body Bimanual Coordination in Immersive Environments",
			"authors": [
				{
					"name": "James Smith",
					"me": true
				},
				{
					"name": "Xinyun Cao"
				},
				{
					"name": "Adolfo Ramirez-Aristizabal"
				},
				{
					"name": "Bjoern Hartmann"
				}
			],
			"description": "This empirical study explores how users can control multiple virtual bodies in VR to perform coordinated tasks, such as passing objects between a first-person and third-person body. A study with 19 participants showed that people can successfully complete complex interactions using two bodies. The results highlight user strategies and how people perceive their virtual embodiment, which we connect to concepts in embodied cognition and phenomenology.",
			"venue": "DIS 23",
			"acmdl": "https://dl.acm.org/doi/10.1145/3563657.3596082",
			"doi": "https://doi.org/10.1145/3563657.3596082",
			"pdf": "papers/smith_dbbc_23.pdf",
			"video": "https://youtu.be/fWsPGINexYI",
			"images": [
				{
					"url": "img/dbbc/Figure1.png",
					"alt": "Three part diagram showing two figures handing an object between them. The first is labeled Selection and shows a red figure pointing at an object. The second is labeled Manipulation and shows the red figure holding the object in front of a blue figure. The third is labeled Coordination and shows the blue figure grabbing the object.",
					"size": "9"
				}
			],
			"image": {
				"url": "img/dbbc/dbbc-selection-coordination.png",
				"alt": "Two part illustration showing two figures in VR coordination to hand off an object between them."
			}
		},
		{
			"title": "VR or Not? Investigating Interface Type and User Strategies for Interactive Design Space Exploration",
			"authors": [
				{
					"name": "Ananya Nandy"
				},
				{
					"name": "James Smith",
					"me": true
				},
				{
					"name": "Nicholas Jennings"
				},
				{
					"name": "Mike Kuniavsky"
				},
				{
					"name": "Kosa Goucher-Lambert"
				},
				{
					"name": "Bjoern Hartmann"
				}
			],
			"description": "Designers explore large design spaces differently depending on whether they use 2D or VR interfaces, with each offering unique advantages. A study with 28 participants found that 2D interfaces are better for broadly comparing multiple options, while VR helps users deeply understand individual designs in context. These findings suggest that interface choice influences design workflows, balancing efficiency in comparison with immersion in evaluation.",
			"venue": "ICED 23",
			"pubonly": true,
			"pdf": "papers/nandy_vr_or_not_23.pdf",
			"acmdl": "https://www.cambridge.org/core/journals/proceedings-of-the-design-society/article/vr-or-not-investigating-interface-type-and-user-strategies-for-interactive-design-space-exploration/BFA6CE13915DD825D04B70806CCB70A3",
			"doi": "https://doi.org/10.1017/pds.2023.386",
			"images": [
			],
			"image": {
				"url": "img/vr-or-not.png",
				"alt": "Screenshot of a 2d design interface next to a 3d design interface."
			}
		},
		{
			"title": "GeneratiVR: Spatial Interactions in Virtual Reality to Explore Generative Design Spaces",
			"authors": [
				{
					"name": "Nicholas Jennings"
				},
				{
					"name": "Ananya Nandy"
				},
				{
					"name": "Xinyi Zhu"
				},
				{
					"name": "Yuting Wang"
				},
				{
					"name": "Fanping Sui"
				},
				{
					"name": "James Smith",
					"me": true
				},
				{
					"name": "Bjoern Hartmann"
				}
			],
			"description": "Computational design tools can automatically generate large quantities of viable designs for a given design problem. This raises the challenge of how to enable designers to efficiently and effectively evaluate and select preferred designs from a large set of alternatives. In GeneratiVR, we present two novel interaction techniques to address this challenge, by leveraging Virtual Reality for rich, spatial user input. With these interaction methods, users can directly manipulate designs or demonstrate desired design functionality. The interactions allow users to rapidly filter through an expansive design space to specify or find their preferred designs.",
			"venue": "CHI 2022 EA",
			"pdf": "papers/jennings_generativr_22.pdf",
			"acmdl": "https://dl.acm.org/doi/abs/10.1145/3491101.3519616",
			"doi": "https://doi.org/10.1145/3491101.3519616",
			"video": "https://youtu.be/qHSx5f6fRts",
			"poster": true,
			"images": [
				{
					"url": "img/gvr/gvr-system.png",
					"alt": "User pointing a cell phone at a cup.",
					"size": "5"
				},
				{
					"url": "img/gvr/gvr-filters.png",
					"alt": "Cup with holographic cube overlayed. Shows an interface for placing holocubes and also collecting training images.",
					"size": "5"
				},
				{
					"url": "img/gvr/gvr-screenshot.png",
					"alt": "Depicts the 3d reprojection process from holocube to 2d image annotation.",
					"size": "2"
				}
			],
			"image": {
				"url": "img/gvr/gvr-filters.png",
				"alt": "Cup with holographic cube overlayed. Shows an interface for placing holocubes and also collecting training images."
			}
		},
		{
			"title": "Composing Flexibly-Organized Step-by-Step Tutorials from Linked Source Code, Snippets, and Outputs",
			"authors": [
				{
					"name": "Andrew Head"
				},
				{
					"name": "Jason Jiang"
				},
				{
					"name": "James Smith",
					"me": true
				},
				{
					"name": "Marti A. Hearst"
				},
				{
					"name": "Björn Hartmann"
				}
			],
			"description": "Programming tutorials often struggle with maintaining consistency between code snippets, outputs, and explanations as content evolves. Interviews with 12 authors and analysis of 200 tutorials revealed these challenges, leading to the development of Torii—a tool that automates synchronizing code and outputs during tutorial creation. Our study showed Torii effectively addresses these pain points, offering insights for future tools aimed at simplifying technical documentation workflows.",
			"venue": "CHI 2020",
			"pdf": "papers/head_tutorial_authoring_20.pdf",
			"video": "https://www.youtube.com/watch?v=tTzrBds7Nns",
			"award": "honorable-mention",
			"pubonly": true,
			"acmdl": "https://dl.acm.org/doi/abs/10.1145/3313831.3376798",
			"doi": "https://doi.org/10.1145/3313831.3376798",
			"images": [
				{
					"url": "img/torii/tutorial-authoring.jpg",
					"alt": "Diagram showing program execution order.",
					"size": "3"
				}
			],
			"image": {
				"url": "img/torii.png",
				"alt": "A diagram showing how code is executed in a computational notebook, and how showing code in tutorials is different."
			}
		},
		{
			"title": "LabelAR: A Spatial Guidance Interface for Fast Computer Vision Image Collection",
			"authors": [
				{
					"name": "Mike Laielli*"
				},
				{
					"name": "James Smith*",
					"me": true
				},
				{
					"name": "Giscard Biamby*"
				},
				{
					"name": "Trevor Darrell"
				},
				{
					"name": "Bjoern Hartmann"
				}
			],
			"description": "Training custom computer vision models often requires time-consuming manual data collection and labeling. LabelAR simplifies this by using augmented reality (AR) to let users mark objects with 3D bounding boxes, then automatically captures labeled images from diverse angles as the user moves the camera around. A study with 12 participants showed LabelAR achieves better model accuracy with less effort compared to traditional methods, streamlining the process of creating datasets for object instance recognition.",
			"venue": "UIST 2019",
			"pdf": "papers/laielli_labelar_19.pdf",
			"acmdl": "https://dl.acm.org/doi/abs/10.1145/3332165.3347927",
			"doi": "https://doi.org/10.1145/3332165.3347927",
			"video": "https://www.youtube.com/watch?v=WwYAClL6MlU",
			"images": [
				{
					"url": "img/labelar/user.jpg",
					"alt": "User pointing a cell phone at a cup.",
					"size": "3"
				},
				{
					"url": "img/labelar/labelar-interfaces.jpg",
					"alt": "Cup with holographic cube overlayed. Shows an interface for placing holocubes and also collecting training images.",
					"size": "4"
				},
				{
					"url": "img/labelar/bbox-projection.jpg",
					"alt": "Depicts the 3d reprojection process from holocube to 2d image annotation.",
					"size": "5"
				}
			],
			"image": {
				"url": "img/labelar/bbox-projection.jpg",
				"alt": "Depicts the 3d reprojection process from holocube to 2d image annotation."
			}
		},
		{
			"title": "HindSight: Enhancing Spatial Awareness by Sonifying Detected Objects in Real-Time 360-Degree Video",
			"description": "HindSight increases the environmental awareness of cyclists by warning them of vehicles approaching from outside their visual field. A panoramic camera mounted on a bicycle helmet streams real-time, 360-degree video to a laptop running YOLOv2, a neural object detector designed for real-time use. Detected vehicles are passed through a filter bank to find the most relevant. Resulting vehicles are sonified using bone conduction headphones, giving cyclists added margin to react.",
			"authors": [
				{
					"name": "Eldon Schoop"
				},
				{
					"name": "James Smith",
					"me": true
				},
				{
					"name": "Bjoern Hartmann"
				}
			],
			"venue": "CHI 2018",
			"pdf": "papers/schoop_hindsight_18.pdf",
			"video": "https://www.youtube.com/watch?v=n5ni9usMkD4",
			"acmdl": "https://dl.acm.org/doi/abs/10.1145/3173574.3173717",
			"doi": "https://doi.org/10.1145/3173574.3173717",
			"images": [
				{
					"url": "img/hindsight/donut_diagram_s.jpg",
					"alt": "HindSight Sensing Diagram",
					"size": "4"
				},
				{
					"url": "img/hindsight/lbd_s.jpg",
					"alt": "System Diagram",
					"size": "4"
				},
				{
					"url": "img/hindsight/filtering_stages_s.jpg",
					"alt": "Filtering Stages",
					"size": "4"
				}
			],
			"image": {
				"url": "img/hindsight/filtering_stages_s.jpg",
				"alt": "Filtering Stages"
			}
		}
	],
	"posters": [
		{
			"title": "GeneratiVR: Spatial Interactions in Virtual Reality to Explore Generative Design Spaces",
			"authors": "Nicholas Jennings, Ananya Nandy, Xinyi Zhu, Yuting Wang, Fanping Sui, <em>James Smith</em>, Bjoern Hartmann",
			"description": "Computational design tools can automatically generate large quantities of viable designs for a given design problem. This raises the challenge of how to enable designers to efficiently and effectively evaluate and select preferred designs from a large set of alternatives. In GeneratiVR, we present two novel interaction techniques to address this challenge, by leveraging Virtual Reality for rich, spatial user input. With these interaction methods, users can directly manipulate designs or demonstrate desired design functionality. The interactions allow users to rapidly filter through an expansive design space to specify or find their preferred designs.",
			"venue": "CHI 2022 EA",
			"paper": "papers/generativr.pdf",
			"acmdl": "https://dl.acm.org/doi/abs/10.1145/3491101.3519616",
			"video": "https://youtu.be/qHSx5f6fRts",
			"images": [
				{
					"url": "img/gvr/gvr-system.png",
					"alt": "Diagram showing flow of data in system.",
					"size": "5"
				},
				{
					"url": "img/gvr/gvr-filters.png",
					"alt": "Depiction of the interactions used in each type of filter.",
					"size": "5"
				},
				{
					"url": "img/gvr/gvr-screenshot.png",
					"alt": "Screenshot of the VR interface",
					"size": "2"
				}
			],
			"image": "img/gvr/gvr-filters.png"
		}
	],
	"projects-old": [
		{
			"title": "VirtuWheel",
			"description": "A major problem with virtual reality is the lack of feedback the user receives during interactions with virtual objects. One solution to this is to use physical props to provide haptic feedback, but this still lacks visual feedback cues that may be helpful or necessary for some tasks. This can negatively affect the user’s sense of embodiment in the virtual space. <i>VirtuWheel</i> provides visual feedback during virtual object interactions by using touch sensors on the surface a steering wheel controller which we can use to visualize interactions with the steering wheel in VR.",
			"authors": "<em>James Smith</em>, Raquel Izquierdo, Caitlin Kwan, Greg Giebel",
			"venue": "Completed for EE249A - Embedded Systems",
			"report": "reports/VirtuWheel.pdf",
			"video": "https://www.youtube.com/watch?v=wzfDXCfKiVs",
			"images": [
				{
					"url": "img/virtuwheel/system_s.png",
					"alt": "Components of the VirtuWheel system",
					"size": "4"
				},
				{
					"url": "img/virtuwheel/vr_s.png",
					"alt": "Visualization of the interaction in virtual reality",
					"size": "4"
				},
				{
					"url": "img/virtuwheel/signal_s.png",
					"alt": "Example of what a signal from our system looks like",
					"size": "4"
				}
			]
		}
	],
	"jobs": [
		{
			"title": "Research Scientist", 
			"location": "Microsoft Research", 
			"url": "https://www.microsoft.com/en-us/research/group/epic/", 
			"urltext": "EPIC Group",
			"time": "May 2021 - August 2021"
		},
		{
			"title": "VR Software Engineer", 
			"location": "UC Berkeley", 
			"url": "http://jacobsinstitute.berkeley.edu", 
			"urltext": "Jacobs Institute for Design Innovation",
			"time": "January 2019 - May 2019"
		},
		{
			"title": "Gameplay Programmer",
			"location": "Sony Online Entertainment",
			"time": "June 2008 - October 2010"
		},
		{
			"title": "Gameplay Programmer",
			"location": "Zombie Studios",
			"time": "May 2005 - June 2008"
		}
	],
	"mentees": [
		{
			"name": "Anya Agarwal",
			"url": "http://anya0402.github.io/",
			"outcome": "UPenn MS"
		},
		{
			"name": "Nicholas Jennings",
			"url": "https://nicholasjj.github.io/",
			"outcome": "UC Berkeley MS"
		},
		{
			"name": "Xinyun Cao",
			"url": "https://xinyun-cao.github.io/",
			"outcome": "Univ. of Michigan PhD"
		},
		{
			"name": "Frederick Kim",
			"url": "https://www.linkedin.com/in/kwk/",
			"outcome": "UC Berkeley MS"
		},
		{
			"name": "Woojin Ko",
			"url": "https://www.woojinko.com/",
			"outcome": "Cornell Tech PhD"
		}
	],
	"teaching": [
		{
			"title": "CS 160: User Interface Software and Technology",
			"location": "UC Berkeley",
			"semester": "Spring 2025",
			"role": "GSI"
		},
		{
			"title": "CS 260B - HCI Research",
			"location": "UC Berkeley",
			"semester": "Fall 2024",
			"role": "GSI"
		},
		{
			"title": "CS 160: User Interface Software and Technology",
			"location": "UC Berkeley",
			"semester": "Summer 2024",
			"role": "GSI"
		},
		{
			"title": "CS 294-137 - Immersive Computing and Virtual Reality",
			"location": "UC Berkeley",
			"semester": "Fall 2023",
			"role": "GSI"
		},
		{
			"title": "EECS 149/249A - Embedded Systems",
			"location": "UC Berkeley",
			"semester": "Spring 2021",
			"role": "GSI",
			"award": "Outstanding GSI Award"
		},
		{
			"title": "Peer-Led Team Learning - Physics",
			"location": "Sierra College",
			"semester": "Spring 2015",
			"role": "Peer Mentor"
		},
		{
			"title": "Peer-Led Team Learning - Physics",
			"location": "Sierra College",
			"semester": "Fall 2014",
			"role": "Peer Mentor"
		}
	]
}
