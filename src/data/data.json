{
	"social_media": [
		{
			"url": "mailto:james.smith deletethis at berkeley.edu",
			"class": "fas fa-envelope",
			"name": "email"
		},
				{
			"url": "https://github.com/jamesdsmith",
			"class": "fab fa-github",
			"name": "github"
		},
		{
			"url": "https://linkedin.com/in/jdsmithcoder",
			"class": "fab fa-linkedin",
			"name": "linked in"
		},
		{
			"url": "https://scholar.google.com/citations?user=qm5jHocAAAAJ&hl=en",
			"class": "ai ai-google-scholar-square",
			"name": "google scholar"
		}
	],
	"projects": [
		{
			"title": "What's the Game, then? Opportunities and Challenges for Runtime Behavior Generation",
			"authors": [
				{
					"name": "Nicholas Jennings"
				},
				{
					"name": "Han Wang"
				},
				{
					"name": "Isabel Li"
				},
				{
					"name": "James Smith",
					"me": true
				},
				{
					"name": "Bjoern Hartmann"
				}
			],
			"description": "Dual Body Bimanual Coordination is an empirical study where users control and interact with the world through two bodies in virtual reality simultaneously. Users select and manipulate objects to perform a coordinated handoff between two bodies under the control of a single user. We investigate people's performance in doing this task, classify strategies for how they chose to coordinate their hands, report on sense of embodiment during the scenario, and share qualitative observations about user body schema.",
			"venue": "UIST 24",
			"pubonly": true,
			"award": "best-paper",
			"acmdl": "https://dl.acm.org/doi/10.1145/3654777.3676358",
			"doi": "https://doi.org/10.1145/3654777.3676358",
			"pdf": "papers/gromit.pdf",
			"images": []
		},
		{
			"title": "Dual Body Bimanual Coordination in Immersive Environments",
			"authors": [
				{
					"name": "James Smith",
					"me": true
				},
				{
					"name": "Xinyun Cao"
				},
				{
					"name": "Adolfo Ramirez-Aristizabal"
				},
				{
					"name": "Bjoern Hartmann"
				}
			],
			"description": "Dual Body Bimanual Coordination is an empirical study where users control and interact with the world through two bodies in virtual reality simultaneously. Users select and manipulate objects to perform a coordinated handoff between two bodies under the control of a single user. We investigate people's performance in doing this task, classify strategies for how they chose to coordinate their hands, report on sense of embodiment during the scenario, and share qualitative observations about user body schema.",
			"venue": "DIS 23",
			"acmdl": "https://dl.acm.org/doi/10.1145/3563657.3596082",
			"doi": "https://doi.org/10.1145/3563657.3596082",
			"pdf": "papers/dbbc.pdf",
			"video": "https://youtu.be/fWsPGINexYI",
			"images": [
				{
					"url": "img/dbbc/Figure1.png",
					"alt": "Three part diagram showing two figures handing an object between them. The first is labeled Selection and shows a red figure pointing at an object. The second is labeled Manipulation and shows the red figure holding the object in front of a blue figure. The third is labeled Coordination and shows the blue figure grabbing the object.",
					"size": "9"
				}
			]
		},
		{
			"title": "VR or Not? Investigating Interface Type and User Strategies for Interactive Design Space Exploration",
			"authors": [
				{
					"name": "Ananya Nandy"
				},
				{
					"name": "James Smith",
					"me": true
				},
				{
					"name": "Nicholas Jennings"
				},
				{
					"name": "Mike Kuniavsky"
				},
				{
					"name": "Kosa Goucher-Lambert"
				},
				{
					"name": "Bjoern Hartmann"
				}
			],
			"description": "Computational design tools can automatically generate large quantities of viable designs for a given design problem. This raises the challenge of how to enable designers to efficiently and effectively evaluate and select preferred designs from a large set of alternatives. In GeneratiVR, we present two novel interaction techniques to address this challenge, by leveraging Virtual Reality for rich, spatial user input. With these interaction methods, users can directly manipulate designs or demonstrate desired design functionality. The interactions allow users to rapidly filter through an expansive design space to specify or find their preferred designs.",
			"venue": "ICED 23",
			"pubonly": true,
			"pdf": "papers/vr-or-not.pdf",
			"acmdl": "https://www.cambridge.org/core/journals/proceedings-of-the-design-society/article/vr-or-not-investigating-interface-type-and-user-strategies-for-interactive-design-space-exploration/BFA6CE13915DD825D04B70806CCB70A3",
			"doi": "https://doi.org/10.1017/pds.2023.386",
			"images": [
			],
			"image": {
				"url": "img/gvr/gvr-filters.png",
				"alt": "Cup with holographic cube overlayed. Shows an interface for placing holocubes and also collecting training images."
			}
		},
		{
			"title": "GeneratiVR: Spatial Interactions in Virtual Reality to Explore Generative Design Spaces",
			"authors": [
				{
					"name": "Nicholas Jennings"
				},
				{
					"name": "Ananya Nandy"
				},
				{
					"name": "Xinyi Zhu"
				},
				{
					"name": "Yuting Wang"
				},
				{
					"name": "Fanping Sui"
				},
				{
					"name": "James Smith",
					"me": true
				},
				{
					"name": "Bjoern Hartmann"
				}
			],
			"description": "Computational design tools can automatically generate large quantities of viable designs for a given design problem. This raises the challenge of how to enable designers to efficiently and effectively evaluate and select preferred designs from a large set of alternatives. In GeneratiVR, we present two novel interaction techniques to address this challenge, by leveraging Virtual Reality for rich, spatial user input. With these interaction methods, users can directly manipulate designs or demonstrate desired design functionality. The interactions allow users to rapidly filter through an expansive design space to specify or find their preferred designs.",
			"venue": "CHI 2022 Extended Abstracts",
			"pdf": "papers/generativr.pdf",
			"acmdl": "https://dl.acm.org/doi/abs/10.1145/3491101.3519616",
			"doi": "https://doi.org/10.1145/3491101.3519616",
			"video": "https://youtu.be/qHSx5f6fRts",
			"poster": true,
			"images": [
				{
					"url": "img/gvr/gvr-system.png",
					"alt": "User pointing a cell phone at a cup.",
					"size": "5"
				},
				{
					"url": "img/gvr/gvr-filters.png",
					"alt": "Cup with holographic cube overlayed. Shows an interface for placing holocubes and also collecting training images.",
					"size": "5"
				},
				{
					"url": "img/gvr/gvr-screenshot.png",
					"alt": "Depicts the 3d reprojection process from holocube to 2d image annotation.",
					"size": "2"
				}
			]
		},
		{
			"title": "Composing Flexibly-Organized Step-by-Step Tutorials from Linked Source Code, Snippets, and Outputs",
			"authors": [
				{
					"name": "Andrew Head"
				},
				{
					"name": "Jason Jiang"
				},
				{
					"name": "James Smith",
					"me": true
				},
				{
					"name": "Marti A. Hearst"
				},
				{
					"name": "Björn Hartmann"
				}
			],
			"description": "Programming tutorials are a pervasive, versatile medium for teaching programming. In this paper, we report on the content and structure of programming tutorials, the pain points authors experience in writing them, and a design for a tool to help improve this process. An interview study with 12 experienced tutorial authors found that they construct documents by interleaving code snippets with text and illustrative outputs. It also revealed that authors must often keep the related artifacts of source programs, snippets, and outputs consistent as a program evolves. A content analysis of 200 frequentlyreferenced tutorials on the web also found that most tutorials contain related artifacts—duplicate code and outputs generated from snippets—that an author would need to keep consistent with each other. To address these needs, we designed a tool called Torii with novel authoring capabilities. An in-lab study showed that tutorial authors can successfully use the tool for the unique affordances identified, and provides guidance for designing future tools for tutorial authoring.",
			"venue": "CHI 2020",
			"pdf": "papers/tutorial-authoring.pdf",
			"video": "https://www.youtube.com/watch?v=tTzrBds7Nns",
			"award": "honorable-mention",
			"pubonly": true,
			"acmdl": "https://dl.acm.org/doi/abs/10.1145/3313831.3376798",
			"doi": "https://doi.org/10.1145/3313831.3376798",
			"images": [
				{
					"url": "img/torii/tutorial-authoring.jpg",
					"alt": "Diagram showing program execution order.",
					"size": "3"
				}
			]
		},
		{
			"title": "LabelAR: A Spatial Guidance Interface for Fast Computer Vision Image Collection",
			"authors": [
				{
					"name": "Mike Laielli*"
				},
				{
					"name": "James Smith*",
					"me": true
				},
				{
					"name": "Giscard Biamby*"
				},
				{
					"name": "Trevor Darrell"
				},
				{
					"name": "Bjoern Hartmann"
				}
			],
			"description": "Computer vision is applied in an ever expanding range of applications, many of which require custom training data to perform well. We present a novel interface for rapid collection and labeling of training images to improve computer vision based object detectors. <i>LabelAR</i> leverages the spatial tracking capabilities of an AR-enabled camera, allowing users to place persistent bounding volumes that stay centered on real-world objects. The interface then guides the user to move the camera to cover a wide variety of viewpoints. We eliminate the need for post-hoc manual labeling of images by automatically projecting 2D bounding boxes around objects in the images as they are captured from AR-marked viewpoints. In a user study with 12 participants, <i>LabelAR</i> significantly outperforms existing approaches in terms of the trade-off between model performance and collection time.",
			"venue": "UIST 2019",
			"pdf": "papers/LabelAR.pdf",
			"acmdl": "https://dl.acm.org/doi/abs/10.1145/3332165.3347927",
			"doi": "https://doi.org/10.1145/3332165.3347927",
			"video": "https://www.youtube.com/watch?v=WwYAClL6MlU",
			"images": [
				{
					"url": "img/labelar/user.jpg",
					"alt": "User pointing a cell phone at a cup.",
					"size": "3"
				},
				{
					"url": "img/labelar/labelar-interfaces.jpg",
					"alt": "Cup with holographic cube overlayed. Shows an interface for placing holocubes and also collecting training images.",
					"size": "4"
				},
				{
					"url": "img/labelar/bbox-projection.jpg",
					"alt": "Depicts the 3d reprojection process from holocube to 2d image annotation.",
					"size": "5"
				}
			]
		},
		{
			"title": "HindSight: Enhancing Spatial Awareness by Sonifying Detected Objects in Real-Time 360-Degree Video",
			"description": "<i>HindSight</i> increases the environmental awareness of cyclists by warning them of vehicles approaching from outside their visual field. A panoramic camera mounted on a bicycle helmet streams real-time, 360-degree video to a laptop running YOLOv2, a neural object detector designed for real-time use. Detected vehicles are passed through a filter bank to find the most relevant. Resulting vehicles are sonified using bone conduction headphones, giving cyclists added margin to react.",
			"authors": [
				{
					"name": "Eldon Schoop"
				},
				{
					"name": "James Smith",
					"me": true
				},
				{
					"name": "Bjoern Hartmann"
				}
			],
			"venue": "CHI 2018",
			"pdf": "papers/HindSight.pdf",
			"video": "https://www.youtube.com/watch?v=n5ni9usMkD4",
			"acmdl": "https://dl.acm.org/doi/abs/10.1145/3173574.3173717",
			"doi": "https://doi.org/10.1145/3173574.3173717",
			"images": [
				{
					"url": "img/hindsight/donut_diagram_s.jpg",
					"alt": "HindSight Sensing Diagram",
					"size": "4"
				},
				{
					"url": "img/hindsight/lbd_s.jpg",
					"alt": "System Diagram",
					"size": "4"
				},
				{
					"url": "img/hindsight/filtering_stages_s.jpg",
					"alt": "Filtering Stages",
					"size": "4"
				}
			]
		}
	],
	"posters": [
		{
			"title": "GeneratiVR: Spatial Interactions in Virtual Reality to Explore Generative Design Spaces",
			"authors": "Nicholas Jennings, Ananya Nandy, Xinyi Zhu, Yuting Wang, Fanping Sui, <em>James Smith</em>, Bjoern Hartmann",
			"description": "Computational design tools can automatically generate large quantities of viable designs for a given design problem. This raises the challenge of how to enable designers to efficiently and effectively evaluate and select preferred designs from a large set of alternatives. In GeneratiVR, we present two novel interaction techniques to address this challenge, by leveraging Virtual Reality for rich, spatial user input. With these interaction methods, users can directly manipulate designs or demonstrate desired design functionality. The interactions allow users to rapidly filter through an expansive design space to specify or find their preferred designs.",
			"venue": "CHI 2022 EA",
			"paper": "papers/generativr.pdf",
			"acmdl": "https://dl.acm.org/doi/abs/10.1145/3491101.3519616",
			"video": "https://youtu.be/qHSx5f6fRts",
			"images": [
				{
					"url": "img/gvr/gvr-system.png",
					"alt": "Diagram showing flow of data in system.",
					"size": "5"
				},
				{
					"url": "img/gvr/gvr-filters.png",
					"alt": "Depiction of the interactions used in each type of filter.",
					"size": "5"
				},
				{
					"url": "img/gvr/gvr-screenshot.png",
					"alt": "Screenshot of the VR interface",
					"size": "2"
				}
			],
			"image": "img/gvr/gvr-filters.png"
		}
	],
	"projects-old": [
		{
			"title": "VirtuWheel",
			"description": "A major problem with virtual reality is the lack of feedback the user receives during interactions with virtual objects. One solution to this is to use physical props to provide haptic feedback, but this still lacks visual feedback cues that may be helpful or necessary for some tasks. This can negatively affect the user’s sense of embodiment in the virtual space. <i>VirtuWheel</i> provides visual feedback during virtual object interactions by using touch sensors on the surface a steering wheel controller which we can use to visualize interactions with the steering wheel in VR.",
			"authors": "<em>James Smith</em>, Raquel Izquierdo, Caitlin Kwan, Greg Giebel",
			"venue": "Completed for EE249A - Embedded Systems",
			"report": "reports/VirtuWheel.pdf",
			"video": "https://www.youtube.com/watch?v=wzfDXCfKiVs",
			"images": [
				{
					"url": "img/virtuwheel/system_s.png",
					"alt": "Components of the VirtuWheel system",
					"size": "4"
				},
				{
					"url": "img/virtuwheel/vr_s.png",
					"alt": "Visualization of the interaction in virtual reality",
					"size": "4"
				},
				{
					"url": "img/virtuwheel/signal_s.png",
					"alt": "Example of what a signal from our system looks like",
					"size": "4"
				}
			]
		}
	],
	"jobs": [
		{
			"title": "Research Scientist", 
			"location": "Microsoft Research", 
			"url": "https://www.microsoft.com/en-us/research/group/epic/", 
			"urltext": "EPIC Group",
			"time": "May 2021 - August 2021"
		},
		{
			"title": "VR Software Engineer", 
			"location": "UC Berkeley", 
			"url": "http://jacobsinstitute.berkeley.edu", 
			"urltext": "Jacobs Institute for Design Innovation",
			"time": "January 2019 - May 2019"
		},
		{
			"title": "Gameplay Programmer",
			"location": "Sony Online Entertainment",
			"time": "June 2008 - October 2010"
		},
		{
			"title": "Gameplay Programmer",
			"location": "Zombie Studios",
			"time": "May 2005 - June 2008"
		}
	]
}
