<!doctype html>
<html>
	<head>
		<meta charset="UTF-8">
		<!-- Bootstrap CSS -->
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous" />
		<!-- Custom CSS -->
		<link rel="stylesheet" href="style.css" />
		<!-- Font Awesome -->
		<script defer src="https://use.fontawesome.com/releases/v5.0.8/js/all.js"></script>
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
		<title>James Smith</title>
	</head>
	<body>
		<div class="container">
			<header class="row mt-4 mb-4">
				<div id="header" class="col">
					<img class="main-img" src="img/avatar.png" alt="Picture of James Smith" />
					<h1>James Smith</h1>
					
					<a class="social-icon" href="mailto:james.smith deletethis at berkeley.edu"><span aria-hidden="true" class="fas fa-envelope"></span><span class="sr-only">email</span></a>
					
					<a class="social-icon" href="https://github.com/jamesdsmith"><span aria-hidden="true" class="fab fa-github"></span><span class="sr-only">github</span></a>
					
					<a class="social-icon" href="https://linkedin.com/in/jdsmithcoder"><span aria-hidden="true" class="fab fa-linkedin"></span><span class="sr-only">linked in</span></a>
					
					<a class="social-icon" href="https://scholar.google.com/citations?user=qm5jHocAAAAJ&hl=en"><span aria-hidden="true" class="ai ai-google-scholar-square"></span><span class="sr-only">google scholar</span></a>
					
						<p>I am a fourth year Computer Science PhD student at UC Berkeley, advised by professor <a href="https://people.eecs.berkeley.edu/~bjoern/">Bj√∂rn Hartmann</a>. I completed my MS and BS in Electrical Engineering and Computer Science at UC Berkeley.</p>
						<p>During my PhD, I participated in a research internship at Microsoft Research working with <a href="https://www.microsoft.com/en-us/research/people/awilson/">Andy Wilson</a>. I have been supported by Accenture Labs from 2021-2023, working with <a href="https://www.linkedin.com/in/mikek">Mike Kuniavsky</a> and the Digital Experiences team.</p>
						<p>As an undergrad researcher, my mentor was <a href="https://eldon.io/">Eldon Schoop</a>.</p>
						<p>I am always interested in hearing from undergraduates who may be interested in doing AR/VR related research. My goal is to support undergraduate researchers that are interested in pursuing graduate education. These are the students I have mentored: <a href="https://www.linkedin.com/in/nicholasjohnjennings/">Nicholas Jennings</a>, <a href="https://xinyuncao.godaddysites.com/">Xinyun Cao</a>, <a href="https://www.linkedin.com/in/kwk/">Frederick Kim</a> (UC Berkeley MS), <a href="https://www.woojinko.com/">Woojin Ko</a> (Cornell Tech PhD).</p>

				</div>
			</header>
			<main>
			<section id="research" class="row mb-4">
				<div class="col">
					<h2>Research</h2>
					<p>My research interests are broadly in the domain of Virtual Reality and Embodiment. Currently I am focusing on multiple-embodiments, which are interfaces where users control multiple virtual bodies simultaneously. We are studying people's qualitative experiences in such interfaces, and the strategies people develop for controlling these interfaces. My current work seeks to understand how users can interact and coordinate between many bodies and connecting these concepts to Embodied Cognition and Phenomenology.</p>
					<p>I also have interest in how VR can augment design processses, in particular the role that VR should play in early stage prototyping of design. Additionally, I am broadly interested in Graphics and Computer Vision research related to VR.</p>
					<p>Previously I did research at the intersection of augmented reality and computer vision, studying interfaces for collecting in-situ training data and augmenting human awareness using computer vision.</p>
					
					
					<div class="project">
						<div class="row title">
							<div class="col-md-12">
								<h4 class="subsection">Multi-Embodiment in Virtual Reality</h4>
							</div>
						</div>
						<div class="row authors">
							<div class="col-md-12">
								<p class="authors">James Smith, Xinyun Cao, Bjoern Hartmann</p>
							</div>
						</div>
						
						<div class="row venue">
							<div class="col-md-12">
								<p class="venue"></p>
							</div>
						</div>
						
						
						
						<div class="row preview-imgs">
							
							<div class="col-md-8 demo-imgs">
								<img class="research-thumb img-fluid" src="img/multibody/multi-body-system.png" alt="User pointing a cell phone at a cup." />
							</div>
							
						</div>
						<div class="row description">
							<div class="col-md-12">
								<p>We are investigating questions surrounding the coordinated action of multiple simultaneously controlled VR avatars. People's abilities to control multiple bodies in a coordinated manner raises questions about what types of strategies they employ while in this type of interface, how bodies should be placed given certain task configurations, and questions about how users embody the various avatars. <b>This is current ongoing research.</b></p>
								<p>
								
								
								
								
								
								</p>
							</div>
						</div>
					</div>
					
					
					
					
					
					<div class="project">
						<div class="row title">
							<div class="col-md-12">
								<h4 class="subsection">GeneratiVR: Spatial Interactions in Virtual Reality to Explore Generative Design Spaces</h4>
							</div>
						</div>
						<div class="row authors">
							<div class="col-md-12">
								<p class="authors">Nicholas Jennings, Ananya Nandy, Xinyi Zhu, Yuting Wang, Fanping Sui, <em>James Smith</em>, Bjoern Hartmann</p>
							</div>
						</div>
						
						<div class="row venue">
							<div class="col-md-12">
								<p class="venue">CHI 2022 Extended Abstracts</p>
							</div>
						</div>
						
						
						
						<div class="row preview-imgs">
							
							<div class="col-md-5 demo-imgs">
								<img class="research-thumb img-fluid" src="img/gvr/gvr-system.png" alt="User pointing a cell phone at a cup." />
							</div>
							
							<div class="col-md-5 demo-imgs">
								<img class="research-thumb img-fluid" src="img/gvr/gvr-filters.png" alt="Cup with holographic cube overlayed. Shows an interface for placing holocubes and also collecting training images." />
							</div>
							
							<div class="col-md-2 demo-imgs">
								<img class="research-thumb img-fluid" src="img/gvr/gvr-screenshot.png" alt="Depicts the 3d reprojection process from holocube to 2d image annotation." />
							</div>
							
						</div>
						<div class="row description">
							<div class="col-md-12">
								<p>Computational design tools can automatically generate large quantities of viable designs for a given design problem. This raises the challenge of how to enable designers to efficiently and effectively evaluate and select preferred designs from a large set of alternatives. In GeneratiVR, we present two novel interaction techniques to address this challenge, by leveraging Virtual Reality for rich, spatial user input. With these interaction methods, users can directly manipulate designs or demonstrate desired design functionality. The interactions allow users to rapidly filter through an expansive design space to specify or find their preferred designs.</p>
								<p>
								<a href="https://dl.acm.org/doi/abs/10.1145/3491101.3519616">ACM Digital Library</a> - 
								<a href="papers/generativr.pdf">Paper</a> - 
								
								
								<a href="https://youtu.be/qHSx5f6fRts">Video</a>
								</p>
							</div>
						</div>
					</div>
					
					
					
					
					
					<div class="project">
						<div class="row title">
							<div class="col-md-12">
								<h4 class="subsection">LabelAR: A Spatial Guidance Interface for Fast Computer Vision Image Collection</h4>
							</div>
						</div>
						<div class="row authors">
							<div class="col-md-12">
								<p class="authors">Mike Laielli*, <em>James Smith*</em>, Giscard Biamby*, Trevor Darrell, Bjoern Hartmann</p>
							</div>
						</div>
						
						<div class="row venue">
							<div class="col-md-12">
								<p class="venue">UIST 2019</p>
							</div>
						</div>
						
						
						
						<div class="row preview-imgs">
							
							<div class="col-md-3 demo-imgs">
								<img class="research-thumb img-fluid" src="img/labelar/user.jpg" alt="User pointing a cell phone at a cup." />
							</div>
							
							<div class="col-md-4 demo-imgs">
								<img class="research-thumb img-fluid" src="img/labelar/labelar-interfaces.jpg" alt="Cup with holographic cube overlayed. Shows an interface for placing holocubes and also collecting training images." />
							</div>
							
							<div class="col-md-5 demo-imgs">
								<img class="research-thumb img-fluid" src="img/labelar/bbox-projection.jpg" alt="Depicts the 3d reprojection process from holocube to 2d image annotation." />
							</div>
							
						</div>
						<div class="row description">
							<div class="col-md-12">
								<p>Computer vision is applied in an ever expanding range of applications, many of which require custom training data to perform well. We present a novel interface for rapid collection and labeling of training images to improve computer vision based object detectors. <i>LabelAR</i> leverages the spatial tracking capabilities of an AR-enabled camera, allowing users to place persistent bounding volumes that stay centered on real-world objects. The interface then guides the user to move the camera to cover a wide variety of viewpoints. We eliminate the need for post-hoc manual labeling of images by automatically projecting 2D bounding boxes around objects in the images as they are captured from AR-marked viewpoints. In a user study with 12 participants, <i>LabelAR</i> significantly outperforms existing approaches in terms of the trade-off between model performance and collection time.</p>
								<p>
								<a href="https://dl.acm.org/doi/abs/10.1145/3332165.3347927">ACM Digital Library</a> - 
								<a href="papers/LabelAR.pdf">Paper</a> - 
								
								<a href="http://labelar.berkeley.edu">Website</a> - 
								<a href="https://www.youtube.com/watch?v=WwYAClL6MlU">Video</a>
								</p>
							</div>
						</div>
					</div>
					
					
					
					<div class="project">
						<div class="row title">
							<div class="col-md-12">
								<h4 class="subsection">HindSight: Enhancing Spatial Awareness by Sonifying Detected Objects in Real-Time 360-Degree Video</h4>
							</div>
						</div>
						<div class="row authors">
							<div class="col-md-12">
								<p class="authors">Eldon Schoop, <em>James Smith</em>, Bjoern Hartmann</p>
							</div>
						</div>
						
						<div class="row venue">
							<div class="col-md-12">
								<p class="venue">CHI 2018</p>
							</div>
						</div>
						
						
						
						<div class="row preview-imgs">
							
							<div class="col-md-4 demo-imgs">
								<img class="research-thumb img-fluid" src="img/hindsight/donut_diagram_s.jpg" alt="HindSight Sensing Diagram" />
							</div>
							
							<div class="col-md-4 demo-imgs">
								<img class="research-thumb img-fluid" src="img/hindsight/lbd_s.jpg" alt="System Diagram" />
							</div>
							
							<div class="col-md-4 demo-imgs">
								<img class="research-thumb img-fluid" src="img/hindsight/filtering_stages_s.jpg" alt="Filtering Stages" />
							</div>
							
						</div>
						<div class="row description">
							<div class="col-md-12">
								<p><i>HindSight</i> increases the environmental awareness of cyclists by warning them of vehicles approaching from outside their visual field. A panoramic camera mounted on a bicycle helmet streams real-time, 360-degree video to a laptop running YOLOv2, a neural object detector designed for real-time use. Detected vehicles are passed through a filter bank to find the most relevant. Resulting vehicles are sonified using bone conduction headphones, giving cyclists added margin to react.</p>
								<p>
								<a href="https://dl.acm.org/doi/abs/10.1145/3173574.3173717">ACM Digital Library</a> - 
								<a href="papers/HindSight.pdf">Paper</a> - 
								
								
								<a href="https://www.youtube.com/watch?v=n5ni9usMkD4">Video</a>
								</p>
							</div>
						</div>
					</div>
					
					
				</div>
			</section>
			<section id="publication" class="row mb-4">
				<div class="col">
					<h2 class="publication">Publications</h2>
					
					
					
					
					<div class="publication">
						<span class="authors">Ananya Nandy, <em>James Smith</em>, Nicholas Jennings, Mike Kuniavsky, Kosa Goucher-Lambert, Bjoern Hartmann.</span>
						
						
						<span class="title"> 
							
							VR or Not? Investigating Interface Type and User Strategies for Interactive Design Space Exploration,
							
						</span>
						<span class="venue"> ICED 23 (Upcoming)</span>
					</div>
					
					
					
					<div class="publication">
						<span class="authors">Nicholas Jennings, Ananya Nandy, Xinyi Zhu, Yuting Wang, Fanping Sui, <em>James Smith</em>, Bjoern Hartmann.</span>
						
						
						<span class="title"> 
							
							<a href="https://dl.acm.org/doi/abs/10.1145/3491101.3519616">GeneratiVR: Spatial Interactions in Virtual Reality to Explore Generative Design Spaces</a>,
							
						</span>
						<span class="venue"> CHI 2022 Extended Abstracts</span>
					</div>
					
					
					
					<div class="publication">
						<span class="authors">Andrew Head, Jason Jiang, <em>James Smith</em>, Marti A. Hearst, Bj√∂rn Hartmann.</span>
						
						
						<img class="award" src="img/ribbon.png" alt="Nominated for Best Paper Award" />
						
						<span class="title"> 
							
							<a href="https://dl.acm.org/doi/abs/10.1145/3313831.3376798">Composing Flexibly-Organized Step-by-Step Tutorials from Linked Source Code, Snippets, and Outputs</a>,
							
						</span>
						<span class="venue"> CHI 2020</span>
					</div>
					
					
					
					<div class="publication">
						<span class="authors">Mike Laielli*, <em>James Smith*</em>, Giscard Biamby*, Trevor Darrell, Bjoern Hartmann.</span>
						
						
						<span class="title"> 
							
							<a href="https://dl.acm.org/doi/abs/10.1145/3332165.3347927">LabelAR: A Spatial Guidance Interface for Fast Computer Vision Image Collection</a>,
							
						</span>
						<span class="venue"> UIST 2019</span>
					</div>
					
					
					
					<div class="publication">
						<span class="authors">Eldon Schoop, <em>James Smith</em>, Bjoern Hartmann.</span>
						
						
						<span class="title"> 
							
							<a href="https://dl.acm.org/doi/abs/10.1145/3173574.3173717">HindSight: Enhancing Spatial Awareness by Sonifying Detected Objects in Real-Time 360-Degree Video</a>,
							
						</span>
						<span class="venue"> CHI 2018</span>
					</div>
					
					
				</div>
			</section>
			<section id="work" class="row mb-4">
				<div class="col">
					<h2>Industry Experience</h2>
					<p>I am currently looking for research scientist internship positions. Prior to academia, I enjoyed a career as a gameplay programmer in the video game industry. Designing interactive experiences is still an interest of mine.</p>
					<div class="job-list">
						
						<div class="job">
							<h5>Research Scientist, <a href="https://www.microsoft.com/en-us/research/group/epic/">EPIC Group</a> - Microsoft Research</h5>
							<em>May 2021 - August 2021</em>
						</div>
						
						<div class="job">
							<h5>VR Software Engineer, <a href="http://jacobsinstitute.berkeley.edu">Jacobs Institute for Design Innovation</a> - UC Berkeley</h5>
							<em>January 2019 - May 2019</em>
						</div>
						
						<div class="job">
							<h5>Gameplay Programmer - Sony Online Entertainment</h5>
							<em>June 2008 - October 2010</em>
						</div>
						
						<div class="job">
							<h5>Gameplay Programmer - Zombie Studios</h5>
							<em>May 2005 - June 2008</em>
						</div>
						
					</div>
				</div>
			</section>
			<section id="classes" class="row mb-4">
				<div class="col">
					<h2>Classes</h2>
					<p>My undergraduate coursework included the following upper division classes: Signals and Systems, Convex Optimization, Operating Systems, Internet Architecture, CS Theory and Algorithms, Computational Photography, Machine Learning and Computer Vision.</p>
					<p>I've also taken graduate coursework in HCI, AR/VR, Embedded Systems, and Data Visualization. </p>
				</div>
			</section>
			<section id="hobbies" class="row mb-4">
				<div class="col">
					<h2>Hobbies</h2>
					<p>When I'm not doing research or programming, I like to snowboard, go bouldering, ride my motorcycle, or play D&D. I am also an <a href="https://www.worldcubeassociation.org/persons/2017SMIT09">amatuer speedcuber</a>.</p>
				</div>
			</section>
			</main>
		</div>

		<!-- jQuery first, then Popper.js, then Bootstrap JS -->
		<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
		<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
	</body>
</html>